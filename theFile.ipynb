{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, round, max, avg, to_timestamp, dayofweek, date_format, expr\n",
    "from pyspark.sql.types import FloatType\n",
    "import requests\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/05 16:26:47 WARN Utils: Your hostname, SUSHAN resolves to a loopback address: 127.0.1.1; using 172.31.76.59 instead (on interface eth0)\n",
      "23/09/05 16:26:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/05 16:26:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"projectFirst\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stationInfo_df = spark.read.csv(\"data/Fuel_Station_Information.csv\", header=True, inferSchema=True)\n",
    "\n",
    "hourlyPrices_df = spark.read.csv(\"data/Hourly_Gasoline_Prices.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- isSelf: integer (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Fuel_station_manager: string (nullable = true)\n",
      " |-- Petrol_company: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Station_name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitudine: string (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(hourlyPrices_df.printSchema(), stationInfo_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "hourlyPrices_df = hourlyPrices_df.dropDuplicates()\n",
    "stationInfo_df = stationInfo_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null values\n",
    "hourlyPrices_df = hourlyPrices_df.dropna()\n",
    "stationInfo_df = stationInfo_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationInfo_df = stationInfo_df.drop(\"Fuel_station_manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyPrices_df = hourlyPrices_df.withColumn(\"Date\", to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 16:28:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hourlyPrices_df.write.parquet(\"data/cleaned_fuel_prices.parquet\")\n",
    "stationInfo_df.write.parquet(\"data/cleaned_station_info.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_prices_df = spark.read.parquet(\"data/cleaned_fuel_prices.parquet\")\n",
    "station_info_df = spark.read.parquet(\"data/cleaned_station_info.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_euro(usd_price):\n",
    "    # API URL to fetch the exchange rate\n",
    "    api_url = \"https://cdn.jsdelivr.net/gh/fawazahmed0/currency-api@1/latest/currencies/usd/eur.json\"\n",
    "    \n",
    "    # Fetch exchange rate data from the API\n",
    "    response = requests.get(api_url)\n",
    "    exchange_rate_data = response.json()\n",
    "    \n",
    "    # Extract the exchange rate\n",
    "    exchange_rate = exchange_rate_data[\"eur\"]\n",
    "    \n",
    "    # Convert USD to Euro\n",
    "    euro_price = usd_price * exchange_rate\n",
    "    return euro_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+-------------------+------------------+\n",
      "|   Id|isSelf|Price|               Date|Price_Euro_Rounded|\n",
      "+-----+------+-----+-------------------+------------------+\n",
      "| 5079|     1|1.809|2022-01-04 06:39:09|             1.678|\n",
      "|38752|     1|1.804|2022-01-04 07:18:19|             1.674|\n",
      "|51635|     0|1.974|2022-01-04 07:39:23|             1.831|\n",
      "| 6810|     0|2.009|2022-01-04 07:50:04|             1.864|\n",
      "| 4983|     0|1.758|2022-01-04 07:55:36|             1.631|\n",
      "|51790|     1|1.819|2022-01-04 08:50:47|             1.687|\n",
      "|42708|     0|2.064|2022-01-04 08:51:50|             1.915|\n",
      "|48545|     1|1.799|2022-01-04 09:26:35|             1.669|\n",
      "|46455|     0|1.954|2022-01-04 12:14:28|             1.813|\n",
      "|47555|     0|2.048|2022-01-04 12:58:37|               1.9|\n",
      "| 6711|     1|1.819|2022-01-04 13:15:06|             1.687|\n",
      "|31878|     1|1.784|2022-01-04 13:15:25|             1.655|\n",
      "|52822|     1|1.769|2022-01-04 13:19:01|             1.641|\n",
      "|51870|     0|2.054|2022-01-04 13:40:13|             1.905|\n",
      "|46259|     0|1.959|2022-01-04 13:46:46|             1.817|\n",
      "| 3598|     1|1.859|2022-01-04 14:08:03|             1.725|\n",
      "|37707|     1|1.669|2022-01-04 14:28:04|             1.548|\n",
      "|30753|     1|1.799|2022-01-04 14:42:39|             1.669|\n",
      "|46342|     0|1.954|2022-01-04 14:46:33|             1.813|\n",
      "| 9352|     1|1.698|2022-01-04 14:53:36|             1.575|\n",
      "+-----+------+-----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the UDF to convert prices from USD to Euro\n",
    "fuel_prices_df = fuel_prices_df.withColumn(\"Price_Euro\", convert_to_euro(col(\"Price\")))\n",
    "# Round the Euro prices to three decimal units\n",
    "fuel_prices_df = fuel_prices_df.withColumn(\"Price_Euro_Rounded\", round(col(\"Price_Euro\"), 3))\n",
    "\n",
    "# Drop the \"Price_Euro\" column\n",
    "fuel_prices_df = fuel_prices_df.drop(\"Price_Euro\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "fuel_prices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID with Highest Price: 54771\n",
      "Maximum Average Price: 1.835294686861716\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum price\n",
    "max_price = fuel_prices_df.agg(max(\"Price\")).collect()[0][0]\n",
    "\n",
    "# Find the ID with the highest price\n",
    "highest_price_id = fuel_prices_df.filter(col(\"Price\") == max_price).select(\"Id\").first()[0]\n",
    "\n",
    "# Order the data by price in descending order\n",
    "sorted_prices_df = fuel_prices_df.orderBy(col(\"Price\").desc())\n",
    "\n",
    "# Calculate the average price\n",
    "average_price = fuel_prices_df.agg({\"Price\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "# Display the results\n",
    "print(\"ID with Highest Price:\", highest_price_id)\n",
    "print(\"Maximum Average Price:\", average_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 16:28:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/05 16:28:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/05 16:28:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/05 16:28:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/05 16:28:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/05 16:28:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+\n",
      "|Day_of_Week|Sales_Count|Rank|\n",
      "+-----------+-----------+----+\n",
      "|        Mon|     410910|   1|\n",
      "|        Thu|     385288|   2|\n",
      "|        Wed|     375737|   3|\n",
      "|        Tue|     363014|   4|\n",
      "|        Fri|     362752|   5|\n",
      "|        Sat|     308091|   6|\n",
      "|        Sun|     238545|   7|\n",
      "+-----------+-----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fuel_prices_df = fuel_prices_df.withColumn(\"Day_of_Week\", date_format(col(\"Date\"), \"E\"))\n",
    "\n",
    "day_sales_df = fuel_prices_df.groupBy(\"Day_of_Week\").agg(F.count(\"*\").alias(\"Sales_Count\"))\n",
    "\n",
    "# Order the data by sales count in descending order to rank the days\n",
    "window_spec = Window.orderBy(col(\"Sales_Count\").desc())\n",
    "day_sales_ranked_df = day_sales_df.withColumn(\"Rank\", F.rank().over(window_spec))\n",
    "\n",
    "day_sales_ranked_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    return haversine((lat1, lon1), (lat2, lon2), unit=Unit.KILOMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+------+------+\n",
      "|   Fri|   Mon|   Sat|   Sun|   Thu|   Tue|   Wed|\n",
      "+------+------+------+------+------+------+------+\n",
      "|362752|410910|308091|238545|385288|363014|375737|\n",
      "+------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the day of the week from the 'Date' column and create a new column 'Day_of_Week'\n",
    "fuel_prices_df = fuel_prices_df.withColumn(\"Day_of_Week\", date_format(col(\"Date\"), \"E\"))\n",
    "\n",
    "# Pivot the table to get counts for each day of the week\n",
    "pivot_df = fuel_prices_df.groupBy().pivot(\"Day_of_Week\").agg(F.count(\"*\"))\n",
    "\n",
    "# Display the result\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
